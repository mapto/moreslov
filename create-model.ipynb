{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3fdd98d",
   "metadata": {},
   "source": [
    "# Creates incremental models with the specified algorithm (currently Word2Vec or FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77802c9b",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import copy\n",
    "\n",
    "from const import countries as corpora\n",
    "from stemmers import stemmers\n",
    "from algo import algos\n",
    "\n",
    "from util import story_tokenize, collect_tokens\n",
    "from create import tokenize_values, load_source, calc_occurences, annotate_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98eb0d56",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "# tkn = \"ps\"\n",
    "# tkn = \"wnl\"\n",
    "# tkn = 'dummy'\n",
    "# tkn = \"lan\"\n",
    "tkn = \"sb\"\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "algo = \"w2v\"\n",
    "algo = \"ft\"\n",
    "\n",
    "model_dir = \"/home/mapto/models/20230707\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3511247-9c85-4809-a829-91c1b13baa06",
   "metadata": {},
   "source": [
    "## Get bidirectional dictionaries between stemmed values and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4a5aad-e285-45e7-891e-147dcb054b45",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "values, valuesbackref = tokenize_values(tkn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f4b0b-7965-4ea2-a126-9e430dc32a22",
   "metadata": {},
   "source": [
    "## Get the corpora full texts and their tokenisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0e0a7a-218b-4530-abdb-5d5578fd4589",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "fulltexts, tokenized = load_source(stemmers[tkn], corpora)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce797ae-d80a-41b5-9b38-40334d8fd4c2",
   "metadata": {},
   "source": [
    "## Identify occurences of values in texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "211bebda",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "occurences, occurences_tv, occurences_backref = calc_occurences(values, tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a35be-c4a2-4210-88d9-5d6cf4906c3b",
   "metadata": {},
   "source": [
    "## Add explicit reference to values (annotation) next to labels in texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24bd4dd9",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "tokenized = annotate_occurences(tokenized, valuesbackref)\n",
    "# tokenized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e22a2d6",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0e7ee",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "all_tokens = collect_tokens(tokenized)\n",
    "print(f\"tokens: {sum(len(x) for x in all_tokens)}\")\n",
    "model = algos[algo](\n",
    "    sentences=all_tokens,\n",
    "    vector_size=300,\n",
    "    window=8,\n",
    "    min_count=40,\n",
    "    workers=6,\n",
    "    epochs=epochs,\n",
    ")\n",
    "# model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecb386fc",
   "metadata": {},
   "source": [
    "## Update general model with pilot specific models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef612edb",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "p1 = collect_tokens(tokenized, \"G\")\n",
    "p2 = collect_tokens(tokenized, \"I\")\n",
    "p3 = collect_tokens(tokenized, \"P\")\n",
    "print(f\"German tokens (post annotation): {sum(len(x) for x in p1)}\")\n",
    "print(f\"Italian tokens (post annotation): {sum(len(x) for x in p2)}\")\n",
    "print(f\"Portuguese tokens (post annotation): {sum(len(x) for x in p3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7343849a",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "M1 = copy.deepcopy(model)\n",
    "M1.train(p1, epochs=M1.epochs, total_examples=M1.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d718e9",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "M2 = copy.deepcopy(model)\n",
    "M2.train(p2, epochs=M2.epochs, total_examples=M2.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f26fbf",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "M3 = copy.deepcopy(model)\n",
    "M3.train(p3, epochs=M3.epochs, total_examples=M3.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452b589-9d0a-40f8-b836-b83d28d73af5",
   "metadata": {},
   "source": [
    "## Persist models incrementally, i.e. if already exist, create new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707418b1-57bf-4857-b4fe-aac9278ebd5f",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "name_templ = \"{model_dir}/M0.{tkn}.e{epochs}.{algo}.{i}\"\n",
    "base_name = name_templ.format(\n",
    "    model_dir=model_dir, tkn=tkn, epochs=epochs, algo=algo, i=i\n",
    ")\n",
    "while os.path.exists(base_name):\n",
    "    i += 1\n",
    "    base_name = name_templ.format(\n",
    "        model_dir=model_dir, tkn=tkn, epochs=epochs, algo=algo, i=i\n",
    "    )\n",
    "print(base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c681aa-22c8-45ab-8682-5e33289eb755",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "model.save(base_name)\n",
    "\n",
    "m1_name = base_name.replace(\"M0\", \"M1\")\n",
    "M1.save(m1_name)\n",
    "\n",
    "m2_name = base_name.replace(\"M0\", \"M2\")\n",
    "M2.save(m2_name)\n",
    "\n",
    "m3_name = base_name.replace(\"M0\", \"M3\")\n",
    "M3.save(m3_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
